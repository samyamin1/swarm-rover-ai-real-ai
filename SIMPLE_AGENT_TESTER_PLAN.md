# SIMPLE AGENT TESTER PLAN

## ğŸ¯ OBJECTIVE
Create a minimal 2-agent communication test to isolate AI model integration issues before tackling the complex swarm simulation.

## ğŸ“‹ CURRENT ISSUES IDENTIFIED
1. **AI Model Connection**: `Could not connect to Ollama server or list models: 'name'`
2. **Memory Constraints**: Models requiring 6.1 GiB vs 4.1 GiB available
3. **Decision Parsing**: AI returning verbose responses instead of simple commands
4. **Model Loading**: `AttributeError: 'PerceptionBridge' object has no attribute 'llava_model'`

## ğŸš€ SIMPLE AGENT TESTER DESIGN

### **Phase 1: Minimal 2-Agent Communication Test**
```
Agent A (Sender) â†â†’ VLM/LLM â†â†’ Agent B (Receiver)
```

**Components:**
- 2 simple agents (no physics, no pygame)
- Direct Ollama communication test
- Simple message passing
- Clear success/failure metrics

### **Phase 2: Test Scenarios**
1. **Basic Communication**: Agent A sends message, Agent B receives
2. **AI Decision Making**: Agent A asks AI for decision, Agent B executes
3. **Visual Recognition**: Agent A describes scene, Agent B interprets
4. **Memory Test**: Test different model sizes (smollm:135m, phi-2, etc.)

### **Phase 3: Integration Validation**
- Test each AI model individually
- Validate decision parsing
- Measure response times
- Check memory usage

## ğŸ“ FILE STRUCTURE
```
simple_agent_tester/
â”œâ”€â”€ simple_agent.py          # Basic agent class
â”œâ”€â”€ ai_communication.py      # AI model communication
â”œâ”€â”€ test_scenarios.py        # Test scenarios
â”œâ”€â”€ results_logger.py        # Results tracking
â””â”€â”€ run_tests.py            # Main test runner
```

## ğŸ¯ SUCCESS CRITERIA
- [ ] 2 agents can communicate through AI models
- [ ] AI models respond within 5 seconds
- [ ] Decision parsing works correctly
- [ ] Memory usage stays under 4 GiB
- [ ] No connection errors to Ollama

## âš¡ IMPLEMENTATION PLAN

### **Step 1: Create Simple Agent Class**
- Remove all pygame/physics dependencies
- Focus only on AI communication
- Simple state machine (IDLE, SEND, RECEIVE)

### **Step 2: AI Communication Module**
- Test Ollama connection
- Try different models (smollm:135m first)
- Simple prompt/response testing
- Error handling and logging

### **Step 3: Test Scenarios**
- Basic message passing
- AI decision making
- Visual scene interpretation
- Memory usage monitoring

### **Step 4: Results Analysis**
- Compare different models
- Identify bottlenecks
- Document working configurations

## ğŸ”§ TECHNICAL APPROACH

### **Model Priority Order:**
1. `smollm:135m` (smallest, fastest)
2. `phi-2` (medium size)
3. `llava:7b` (if memory allows)

### **Testing Strategy:**
- Start with simplest possible communication
- Gradually increase complexity
- Monitor memory usage in real-time
- Log all errors and responses

## ğŸ¯ BENEFITS OF THIS APPROACH

### **Advantages:**
- âœ… Isolates AI integration issues
- âœ… Faster debugging cycles
- âœ… No pygame/display dependencies
- âœ… Clear success metrics
- âœ… Easy to extend and modify

### **Risk Assessment:**
- âš ï¸ **Low Risk**: Simple test environment
- âš ï¸ **Quick to Implement**: ~2-3 hours
- âš ï¸ **Easy to Debug**: Clear logging
- âš ï¸ **Won't Break Main Code**: Separate module

## ğŸš€ RECOMMENDATION: PROCEED WITH SIMPLE TESTER

**Why this is the best approach:**
1. **Isolation**: We can test AI models without swarm complexity
2. **Speed**: Much faster to debug AI issues
3. **Clarity**: Clear success/failure criteria
4. **Foundation**: Once working, easy to integrate into main simulation

**Estimated Time:** 2-3 hours to create and test
**Risk Level:** Very Low
**Impact:** High (will solve core AI integration issues)

## ğŸ“ NEXT STEPS

1. **Create simple_agent_tester directory**
2. **Implement basic agent communication**
3. **Test Ollama connection with different models**
4. **Validate decision parsing**
5. **Document working configuration**
6. **Integrate findings into main simulation**

**Should we proceed with this plan?** 